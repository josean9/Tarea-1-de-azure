{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Azure SQL...\n",
      "Ejecutando consulta en Azure SQL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josit\\AppData\\Local\\Temp\\ipykernel_17816\\2677076468.py:110: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(SQL_QUERY, conn_azure)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Datos extraídos: 58049 filas\n",
      "Conectando a SQL Server Local...\n",
      "   - Tabla eliminada si existía.\n",
      " Tabla DATAEX.FACT_SALES creada correctamente en SQL Server Local.\n",
      " 58049 filas insertadas en DATAEX.FACT_SALES.\n",
      "\n",
      " ¡Proceso completado!\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#  Conexión a **Azure SQL**\n",
    "AZURE_SERVER = 'uaxmathfis.database.windows.net'\n",
    "AZURE_DATABASE = 'usecases'\n",
    "AZURE_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "azure_conn_str = f\"DRIVER={AZURE_DRIVER};SERVER={AZURE_SERVER};DATABASE={AZURE_DATABASE};Authentication=ActiveDirectoryInteractive\"\n",
    "\n",
    "#  Conexión a **SQL Server LOCAL**\n",
    "LOCAL_SERVER = 'localhost'\n",
    "LOCAL_DATABASE = 'dwh_case1'  \n",
    "LOCAL_DRIVER = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "local_conn_str = f\"DRIVER={LOCAL_DRIVER};SERVER={LOCAL_SERVER};DATABASE={LOCAL_DATABASE};Trusted_Connection=yes;TrustServerCertificate=yes\"\n",
    "\n",
    "#  Consulta SQL en Azure SQL\n",
    "SQL_QUERY = \"\"\"\n",
    "WITH Mediana_KM AS (\n",
    "    SELECT \n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY KM_Ultima_Revision) \n",
    "        OVER () AS Mediana_KM\n",
    "    FROM \n",
    "        DATAEX.[004_rev]\n",
    "    WHERE \n",
    "        KM_Ultima_Revision IS NOT NULL AND KM_Ultima_Revision > 0\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    s.CODE,                          -- Código de la venta\n",
    "    s.Sales_Date,                    -- Fecha de la venta\n",
    "    s.Customer_ID,                   -- ID del cliente\n",
    "    s.Id_Producto,                   -- ID del producto\n",
    "    s.PVP,                           -- Precio de venta al público\n",
    "    s.IMPUESTOS,                     -- Impuestos\n",
    "    s.COSTE_VENTA_NO_IMPUESTOS,      -- Coste de venta sin impuestos\n",
    "    fp.FORMA_PAGO AS Forma_Pago,     -- Descripción de la forma de pago\n",
    "    mv.MOTIVO_VENTA AS Motivo_Venta, -- Descripción del motivo de la venta\n",
    "    t.TIENDA_ID,                     -- ID de la tienda\n",
    "    t.TIENDA_DESC AS Tienda,         -- Descripción de la tienda\n",
    "    e.Car_Age,                       -- Edad del coche (Car Age)\n",
    "    c.QUEJA,                         -- Queja (si existe)\n",
    "    p.Modelo,                        -- Modelo del producto\n",
    "    R.DIAS_DESDE_ULTIMA_REVISION,    -- Días desde la última revisión\n",
    "    COALESCE(\n",
    "        CASE \n",
    "            WHEN R.KM_Ultima_Revision = 0 THEN (SELECT DISTINCT Mediana_KM FROM Mediana_KM)\n",
    "            ELSE R.KM_Ultima_Revision\n",
    "        END, \n",
    "        (SELECT DISTINCT Mediana_KM FROM Mediana_KM)\n",
    "    ) AS KM_Ultima_Revision_Final,   -- KM de la última revisión (con reemplazo de 0 o NULL)\n",
    "    co.Costetransporte,              -- Coste de transporte\n",
    "    co.GastosMarketing,              -- Gastos de marketing\n",
    "    co.Margendistribuidor,           -- Margen del distribuidor\n",
    "    co.Comisión_Marca,               -- Comisión de la marca (si está disponible)\n",
    "    logist.Fue_Lead,                 -- Indica si fue un lead\n",
    "    logist.Lead_Compra,              -- Indica si el lead resultó en compra\n",
    "    logist.Fue_Lead + logist.Lead_Compra AS Lead_Compra_Total, -- Total de leads que resultaron en compra\n",
    "\n",
    "    -- Cálculo del Margen Bruto\n",
    "    ROUND(s.PVP * (co.Margen) * 0.01 * (1 - s.IMPUESTOS / 100), 2) AS Margen_eur_bruto,\n",
    "    \n",
    "    -- Cálculo del Margen Neto\n",
    "    ROUND(\n",
    "        s.PVP * (co.Margen) * 0.01 * (1 - s.IMPUESTOS / 100) \n",
    "        - s.COSTE_VENTA_NO_IMPUESTOS \n",
    "        - (co.Margendistribuidor * 0.01 + co.GastosMarketing * 0.01 - co.Comisión_Marca * 0.01) * s.PVP * (1 - s.IMPUESTOS / 100) \n",
    "        - co.Costetransporte, \n",
    "        2\n",
    "    ) AS Margen_eur\n",
    "\n",
    "FROM \n",
    "    DATAEX.[001_sales] s\n",
    "LEFT JOIN \n",
    "    [DATAEX].[004_rev] R ON s.CODE = R.CODE  -- Join para revisiones\n",
    "LEFT JOIN \n",
    "    DATAEX.[010_forma_pago] fp ON s.FORMA_PAGO_ID = fp.FORMA_PAGO_ID  -- Join para forma de pago\n",
    "LEFT JOIN \n",
    "    DATAEX.[009_motivo_venta] mv ON s.MOTIVO_VENTA_ID = mv.MOTIVO_VENTA_ID  -- Join para motivo de venta\n",
    "LEFT JOIN \n",
    "    DATAEX.[011_tienda] t ON s.TIENDA_ID = t.TIENDA_ID  -- Join para tienda\n",
    "LEFT JOIN \n",
    "    DATAEX.[018_edad] e ON s.CODE = e.CODE  -- Join para Car_Age\n",
    "LEFT JOIN \n",
    "    DATAEX.[008_cac] c ON s.CODE = c.CODE  -- Join para quejas\n",
    "LEFT JOIN \n",
    "    DATAEX.[006_producto] p ON s.Id_Producto = p.Id_Producto  -- Join para producto\n",
    "LEFT JOIN \n",
    "    DATAEX.[007_costes] co ON p.Modelo = co.Modelo  -- Join para costes\n",
    "LEFT JOIN \n",
    "    [usecases].DATAEX.[017_logist] logist ON s.CODE = logist.CODE;  -- Join para Fue_Lead y Lead_Compra\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 🔹 Nombre de la tabla en SQL Server Local\n",
    "NEW_TABLE_NAME = \"DATAEX.FACT_SALES\"\n",
    "\n",
    "try:\n",
    "    #  Conectar a Azure SQL\n",
    "    print(f\"Conectando a Azure SQL...\")\n",
    "    conn_azure = pyodbc.connect(azure_conn_str)\n",
    "    \n",
    "    # 🔹 Ejecutar la consulta en Azure SQL\n",
    "    print(f\"Ejecutando consulta en Azure SQL...\")\n",
    "    df = pd.read_sql(SQL_QUERY, conn_azure)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\" La consulta no devolvió resultados. No se creará la tabla en SQL Server Local.\")\n",
    "    else:\n",
    "        print(f\"   - Datos extraídos: {df.shape[0]} filas\")\n",
    "\n",
    "\n",
    "\n",
    "        #  Convertir NaN en columnas numéricas a 0\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        #  Convertir valores numéricos problemáticos\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype(np.float32)  # Reducir precisión\n",
    "        \n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype(np.int32)  # Evitar valores fuera de rango\n",
    "        \n",
    "        #  Conectar a SQL Server Local\n",
    "        print(f\"Conectando a SQL Server Local...\")\n",
    "        conn_local = pyodbc.connect(local_conn_str)\n",
    "        \n",
    "        with conn_local.cursor() as cursor:\n",
    "            # 🔹 Eliminar la tabla si ya existe\n",
    "            drop_table_sql = f\"DROP TABLE IF EXISTS {NEW_TABLE_NAME}\"\n",
    "            cursor.execute(drop_table_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\"   - Tabla eliminada si existía.\")\n",
    "\n",
    "            # 🔹 Crear la tabla en SQL Server Local con tipos de datos ajustados\n",
    "            create_table_sql = f\"\"\"\n",
    "            CREATE TABLE {NEW_TABLE_NAME} (\n",
    "                {', '.join([\n",
    "                    f'[{col}] FLOAT' if df[col].dtype == np.float32 \n",
    "                    else f'[{col}] INT' if df[col].dtype == np.int32 \n",
    "                    else f'[{col}] NVARCHAR(255)' for col in df.columns\n",
    "                ])}\n",
    "            );\n",
    "            \"\"\"\n",
    "            cursor.execute(create_table_sql)\n",
    "            conn_local.commit()\n",
    "            print(f\" Tabla {NEW_TABLE_NAME} creada correctamente en SQL Server Local.\")\n",
    "\n",
    "            # Insertar los datos en SQL Server Local\n",
    "            placeholders = ', '.join(['?' for _ in df.columns])\n",
    "            insert_sql = f\"INSERT INTO {NEW_TABLE_NAME} VALUES ({placeholders})\"\n",
    "\n",
    "            cursor.fast_executemany = True\n",
    "            cursor.executemany(insert_sql, df.values.tolist())\n",
    "            conn_local.commit()\n",
    "\n",
    "            print(f\" {df.shape[0]} filas insertadas en {NEW_TABLE_NAME}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'conn_azure' in locals():\n",
    "        conn_azure.close()\n",
    "    if 'conn_local' in locals():\n",
    "        conn_local.close()\n",
    "\n",
    "print(\"\\n ¡Proceso completado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "# Configuración de conexiones\n",
    "CONFIG = {\n",
    "    'azure': {\n",
    "        'server': 'uaxmathfis.database.windows.net',\n",
    "        'database': 'usecases',\n",
    "        'driver': '{ODBC Driver 17 for SQL Server}',\n",
    "        'auth': 'Authentication=ActiveDirectoryInteractive'\n",
    "    },\n",
    "    'local': {\n",
    "        'server': 'localhost',\n",
    "        'database': 'dwh_case1',\n",
    "        'driver': '{ODBC Driver 17 for SQL Server}',\n",
    "        'auth': 'Trusted_Connection=yes;TrustServerCertificate=yes'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Especificación de claves\n",
    "TABLE_CONFIG = {\n",
    "    'table_name': 'DATAEX.FACT_SALES',\n",
    "    'primary_key': 'CODE',\n",
    "    'foreign_keys': [\n",
    "        {'column': 'Customer_ID', 'reference_table': 'DATAEX.DIM_CLIENTE', 'reference_column': 'Customer_ID'},\n",
    "        {'column': 'Id_Producto', 'reference_table': 'DATAEX.DIM_PRODUCTO', 'reference_column': 'Id_Producto'},\n",
    "        {'column': 'Sales_Date', 'reference_table': 'DATAEX.DIM_TIEMPO', 'reference_column': 'Date'},\n",
    "        {'column': 'TIENDA_ID', 'reference_table': 'DATAEX.DIM_LUGAR', 'reference_column': 'TIENDA_ID'}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_column_types(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"Detecta los tipos de columna de pandas y los mapea a tipos SQL Server\"\"\"\n",
    "    type_mapping = {\n",
    "        'object': 'NVARCHAR(255)',\n",
    "        'int64': 'INT',\n",
    "        'float64': 'FLOAT',\n",
    "        'datetime64[ns]': 'DATETIME',\n",
    "        'bool': 'BIT'\n",
    "    }\n",
    "    \n",
    "    column_types = {}\n",
    "    for col in df.columns:\n",
    "        pandas_type = str(df[col].dtype)\n",
    "        sql_type = type_mapping.get(pandas_type, 'NVARCHAR(255)')\n",
    "        \n",
    "        # Ajustes especiales para columnas conocidas\n",
    "        if col in ['CODE', 'Id_Producto']:\n",
    "            sql_type = 'NVARCHAR(255)'\n",
    "        elif col in ['PVP', 'IMPUESTOS', 'COSTE_VENTA_NO_IMPUESTOS']:\n",
    "            sql_type = 'DECIMAL(18,2)'\n",
    "            \n",
    "        column_types[col] = sql_type\n",
    "    \n",
    "    return column_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string expression part cannot include a backslash (2164766844.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    create_sql = f\"CREATE TABLE {table_name} (\\n    {',\\n    '.join(columns_sql)}\\n)\"\u001b[0m\n\u001b[1;37m                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "source": [
    "def create_table_with_schema(conn, table_name: str, column_types: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Crea una tabla en SQL Server con el esquema especificado, manejando adecuadamente\n",
    "    la eliminación previa de la tabla si existe y validando los parámetros.\n",
    "    \n",
    "    Args:\n",
    "        conn: Conexión a la base de datos\n",
    "        table_name: Nombre completo de la tabla (incluyendo esquema si es necesario)\n",
    "        column_types: Diccionario con los tipos de datos para cada columna\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: Si los parámetros no son válidos\n",
    "        pyodbc.Error: Si hay errores en la operación SQL\n",
    "    \"\"\"\n",
    "    # Validación de parámetros\n",
    "    if not table_name or '.' not in table_name:\n",
    "        raise ValueError(\"El nombre de la tabla debe incluir el esquema (ej: 'DATAEX.FACT_SALES')\")\n",
    "    \n",
    "    if not column_types:\n",
    "        raise ValueError(\"El diccionario de tipos de columnas no puede estar vacío\")\n",
    "    \n",
    "    # Construcción del SQL para creación de la tabla\n",
    "    columns_sql = []\n",
    "    pk_column = TABLE_CONFIG.get('primary_key', '')\n",
    "    fk_columns = [fk['column'] for fk in TABLE_CONFIG.get('foreign_keys', [])]\n",
    "    \n",
    "    for col, col_type in column_types.items():\n",
    "        # Determinar si la columna debe ser NOT NULL\n",
    "        is_pk = col == pk_column\n",
    "        is_fk = col in fk_columns\n",
    "        nullable = 'NOT NULL' if is_pk or is_fk else 'NULL'\n",
    "        \n",
    "        # Asegurar que las columnas PK y FK tengan tipos válidos\n",
    "        if is_pk and 'varchar' not in col_type.lower() and 'char' not in col_type.lower():\n",
    "            print(f\"Advertencia: La columna PK '{col}' tiene tipo {col_type}. Se recomienda usar tipo texto para claves.\")\n",
    "        \n",
    "        columns_sql.append(f\"[{col}] {col_type} {nullable}\")\n",
    "    \n",
    "    # SQL para creación de tabla\n",
    "    create_sql = f\"\"\"\n",
    "    IF OBJECT_ID('{table_name}', 'U') IS NOT NULL \n",
    "    BEGIN\n",
    "        PRINT 'Eliminando tabla existente {table_name}...'\n",
    "        DROP TABLE {table_name}\n",
    "    END\n",
    "    \n",
    "    CREATE TABLE {table_name} (\n",
    "        {',\\n        '.join(columns_sql)}\n",
    "    );\n",
    "    \n",
    "    PRINT 'Tabla {table_name} creada exitosamente con {len(columns_sql)} columnas';\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Ejecutar en una transacción\n",
    "            cursor.execute(\"BEGIN TRANSACTION\")\n",
    "            \n",
    "            # Eliminar y crear la tabla\n",
    "            cursor.execute(create_sql)\n",
    "            \n",
    "            # Validar que la tabla se creó correctamente\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM INFORMATION_SCHEMA.TABLES \n",
    "                WHERE TABLE_SCHEMA + '.' + TABLE_NAME = '{table_name}'\n",
    "            \"\"\")\n",
    "            if cursor.fetchone()[0] == 0:\n",
    "                raise pyodbc.Error(f\"No se pudo crear la tabla {table_name}\")\n",
    "            \n",
    "            conn.commit()\n",
    "            print(f\"✓ Tabla {table_name} creada exitosamente\")\n",
    "            \n",
    "    except pyodbc.Error as e:\n",
    "        conn.rollback()\n",
    "        error_msg = f\"Error al crear tabla {table_name}: {str(e)}\"\n",
    "        print(f\"✗ {error_msg}\")\n",
    "        raise pyodbc.Error(error_msg) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_constraints(conn, table_name: str):\n",
    "    \"\"\"Añade las PK y FK especificadas en la configuración\"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Añadir PRIMARY KEY\n",
    "            pk_sql = f\"\"\"\n",
    "            ALTER TABLE {table_name}\n",
    "            ADD CONSTRAINT [PK_{table_name.replace('.', '_')}] \n",
    "            PRIMARY KEY ({TABLE_CONFIG['primary_key']})\n",
    "            \"\"\"\n",
    "            cursor.execute(pk_sql)\n",
    "            \n",
    "            # Añadir FOREIGN KEYS\n",
    "            for fk in TABLE_CONFIG['foreign_keys']:\n",
    "                fk_sql = f\"\"\"\n",
    "                ALTER TABLE {table_name}\n",
    "                ADD CONSTRAINT [FK_{table_name.replace('.', '')}_{fk['column']}] \n",
    "                FOREIGN KEY ({fk['column']}) \n",
    "                REFERENCES {fk['reference_table']}({fk['reference_column']})\n",
    "                \"\"\"\n",
    "                cursor.execute(fk_sql)\n",
    "            \n",
    "            conn.commit()\n",
    "        print(\"Constraints añadidas exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al añadir constraints: {str(e)}\")\n",
    "        conn.rollback()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_table(sql_file_path: str):\n",
    "    \"\"\"Función principal que ejecuta todo el proceso de migración\"\"\"\n",
    "    try:\n",
    "        # Leer consulta SQL\n",
    "        with open(sql_file_path, 'r', encoding='utf-8') as f:\n",
    "            sql_query = f.read()\n",
    "        \n",
    "        # Conectar a Azure y obtener datos\n",
    "        with pyodbc.connect(\n",
    "            f\"DRIVER={CONFIG['azure']['driver']};\"\n",
    "            f\"SERVER={CONFIG['azure']['server']};\"\n",
    "            f\"DATABASE={CONFIG['azure']['database']};\"\n",
    "            f\"{CONFIG['azure']['auth']}\"\n",
    "        ) as azure_conn:\n",
    "            df = pd.read_sql(sql_query, azure_conn)\n",
    "            \n",
    "            # Detectar tipos de columnas\n",
    "            column_types = detect_column_types(df)\n",
    "            \n",
    "            # Conectar a SQL local y crear tabla\n",
    "            with pyodbc.connect(\n",
    "                f\"DRIVER={CONFIG['local']['driver']};\"\n",
    "                f\"SERVER={CONFIG['local']['server']};\"\n",
    "                f\"DATABASE={CONFIG['local']['database']};\"\n",
    "                f\"{CONFIG['local']['auth']}\"\n",
    "            ) as local_conn:\n",
    "                # Crear tabla\n",
    "                create_table_with_schema(local_conn, TABLE_CONFIG['table_name'], column_types)\n",
    "                \n",
    "                # Insertar datos\n",
    "                with local_conn.cursor() as cursor:\n",
    "                    # Preparar datos para inserción\n",
    "                    df = df.replace({np.nan: None})\n",
    "                    placeholders = ', '.join(['?'] * len(df.columns))\n",
    "                    insert_sql = f\"INSERT INTO {TABLE_CONFIG['table_name']} VALUES ({placeholders})\"\n",
    "                    \n",
    "                    # Insertar en bloques para mejor performance\n",
    "                    cursor.fast_executemany = True\n",
    "                    cursor.executemany(insert_sql, df.values.tolist())\n",
    "                    local_conn.commit()\n",
    "                    print(f\"{len(df)} registros insertados\")\n",
    "                \n",
    "                # Añadir constraints\n",
    "                add_constraints(local_conn, TABLE_CONFIG['table_name'])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error en el proceso de migración: {str(e)}\")\n",
    "    finally:\n",
    "        print(\"Proceso completado\")\n",
    "\n",
    "# Ejecutar migración\n",
    "if __name__ == \"__main__\":\n",
    "    migrate_table(\"DATAEX.FACT_SALES.sql\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
